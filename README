0. About

Shadowfax Distributed GPGPU Assembly Runtime

Author: Alexander Merritt, merritt.alex@gatech.edu (main author)

Contributors (direct or indirect):
    Magda Slawinska, magg@gatech.edu (some marshaling/RPC code, build system)
    Abhishek Verma (MS - RPC protocol and paper in VTDC'11)
    Vishakha Gupta (original marshaling code)

This implementation of the 'Shadowfax' software is based on GViM developed by
Vishakha Gupta (published in GPUVirt 2009) for virtualized systems, providing a
distributed runtime for applications to transparently utilize GPUs across a
cluster. Code originally in GViM is now scarce in Shadowfax (e.g. the
cuda_packet specification).
 
 
1. Building the code

Dependencies: scons (python), NVIDIA CUDA, GNU GCC

Produce build/bin/ and build/lib/

    $ scons [dbg=1] [network=sdp]

By default, Ethernet is selected.

One instance of 'shadowfax' must be run on each node

    node0 $ cd build/bin/; ./shadowfax main
    node1 $ cd build/bin/; ./shadowfax minion ip-of-main
    node2 $ cd build/bin/; ./shadowfax minion ip-of-main
    [...]

To clean

	$ scons -c

Keeneland - use the GNU Compiler toolchain and point the appropriate cuda
variables in the SConstruct file to the location of the NVIDIA
toolchain/libraries.

2. Applications

i.  Launch shadowfax daemons (master before slaves)
ii. On each node where there is a process belonging to the application, create
an 'assembly hint' file describing to the shadowfax system what properties of
GPUs and assemblies are desired.

mpi=N
policy=[local_first|remote_only]
batch_size=M

N=0 -> not an MPI program
N>0 -> all hints with equal N specify process belongs to same program

M=0 -> use default batch size (8192 packets)
M>0 -> set batch to flush when full or on M RPC packets (M <= 8192)

local_first ->  for each vGPU given to process
                    find first unmapped GPU, else
                    share some local GPU
remote_only ->  for each vGPU given to process
                    find first unmapped remote GPU, else
                    share some remote GPU, else
                    fallback to local_first policy

3. Issues

Compatibility for CUDA 4.x under development

